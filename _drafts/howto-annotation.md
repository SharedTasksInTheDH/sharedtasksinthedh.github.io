---
layout: post
title: "How to Develop Annotation Guidelines"
author:
- reiter
lang: en
excerpt: ""
logo: 
  url: /assets/generic/IMG_1697.jpg
phase: 1
---

*This article describes where to start and how to proceed when developing annotation guidelines. It focuses on the scenario when you're creating new annotation guidelines for a phenomenon that has been described mostly theoretically.*

<div><img src="{{ site.url }}/assets/generic/annotation-workflow.png" style="width:70%" alt="Flowchart depicting the general annotation workflow"/><p class="caption">Figure 1: General annotation workflow</p></div>


Developing annotation guidelines is an iterative process: Once a first version is established, its shortcomings are to be identified and fixed, leading to a second version, which has shortcomings that need to be identified and fixed, etc. This process is displayed schematically in Figure 1. We will describe how to get create a first version, and how to come from one version to the next. The most important idea is that in each round, **a text is annotated by multiple annotators independently**. This is the main device that allows to identify these shortcomings.


## Making Pilot Annotations



## Improving Guidelines
To improve guidelines in this manner, we first need to analyze annotations of the previous "round", before we reformulate/refine the guidelines. This can be done by inspecting the *annotation disagreements*: These are cases in which different annotators made different decisions. Practically, there are two ways of analyzing these disagreements: By discussing them with the annotators, and by counting and inspecting them. 

A discussion with the annotators is fruitful in particular in the first few rounds of the process. Once the annotators are trained and annotation guidelines are maturing, a quantitative view might be sufficient. For the latter, a number of metrics have been established (see [Wikipedia: Inter-rater reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability) for an overview; or Artstein, 2017). Analyzing the inter-annotator-agreement quantitatively gives you a number and allows comparison if you're actually improving your annotation guidelines, but it does not distinguish different kinds of disagreement, although there are reasons for wanting that.

Some of the disagreements will be caused by annotators not paying annotation, or by overlooking something -- annotators are human beings after all. These can be fixed easily, without the need to refine the guidelines. It is good practice to let the annotators fix these mistakes by themselves. 

Other kinds of disagreement can be expected to have impact on the guidelines: If two annotators made different decisions which are both covered by the annotation guidelines, it is likely that the annotation guidelines are contradictory in this aspect. The source of the contradiction should be identified and resolved. 

Many disagreements will be caused when the annotators encounter cases that are not mentioned in the guidelines. In this case, either an existing annotation definition can be applied (perhaps with minor changes), or a new one needs to be defined. 



## References

Artstein, Ron. Inter-annotator Agreement. In: Ide Nancy & Pustejovsky James (eds.) *Handbook of Linguistic Annotation*. Springer, Dordrecht, 2017.
